{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGlh4E7KtbRT"
      },
      "source": [
        "# Chest X-Ray Classification Assignment\n",
        "\n",
        "- Random Forest\n",
        "- Custom CNN\n",
        "- VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkqxwmaCtbRU"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv7gAbScuVqu",
        "outputId": "bd154973-b005-40e8-9b9d-1d156101a109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ImageHash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from ImageHash) (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ImageHash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ImageHash) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ImageHash) (1.15.3)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ImageHash\n",
            "Successfully installed ImageHash-4.3.2\n"
          ]
        }
      ],
      "source": [
        "!uv pip install ImageHash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "21txPfSgtbRV"
      },
      "outputs": [],
      "source": [
        "# Necessary imports\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imagehash import average_hash\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from skimage.feature import hog\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-02T15:29:49.452897Z",
          "iopub.status.busy": "2025-07-02T15:29:49.452665Z",
          "iopub.status.idle": "2025-07-02T15:29:50.466995Z",
          "shell.execute_reply": "2025-07-02T15:29:50.466168Z",
          "shell.execute_reply.started": "2025-07-02T15:29:49.452846Z"
        },
        "id": "cy-suvxetbRV",
        "outputId": "ac910ce7-8630-4215-d90d-2a3a2d6174ef",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- GPU Availability Check (TensorFlow 2.x) ---\n",
            "TensorFlow cannot access a GPU. Running on CPU.\n"
          ]
        }
      ],
      "source": [
        "# Random seed for reproducibility and GPU availability\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "print(\"--- GPU Availability Check (TensorFlow 2.x) ---\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"TensorFlow can access {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}\")\n",
        "else:\n",
        "    print(\"TensorFlow cannot access a GPU. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XlPR8N6tbRW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQy_wASDtbRW"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yElrXQtxtbRW",
        "outputId": "6b58a872-80fb-4ba9-cd91-eec893067200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd())))\n",
        "print(sys.path[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FATmml8Ax-ai",
        "outputId": "0fc025d3-b559-455a-abb9-9d2bf9c9a106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Files and directories in xray_data:\n",
            "total 24\n",
            "drwx------ 2 root root 4096 Apr  3 21:47 01\n",
            "drwx------ 2 root root 4096 Apr  3 21:47 02\n",
            "drwx------ 2 root root 4096 Apr  3 21:47 03\n",
            "drwx------ 2 root root 4096 Apr  3 22:33 test\n",
            "drwx------ 2 root root 4096 Apr  3 22:33 train\n",
            "drwx------ 2 root root 4096 Apr  3 22:33 val\n"
          ]
        }
      ],
      "source": [
        "# prompt: list the xray_data in the google drive\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Replace 'path/to/your/xray_data' with the actual path to your xray_data directory in Google Drive\n",
        "xray_data_path = '/content/drive/My Drive/xray_data/data'\n",
        "\n",
        "if os.path.exists(xray_data_path):\n",
        "  print(\"Files and directories in xray_data:\")\n",
        "  !ls -l \"$xray_data_path\"\n",
        "else:\n",
        "  print(f\"Error: The directory '{xray_data_path}' does not exist in your Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-02T15:29:50.482624Z",
          "iopub.status.busy": "2025-07-02T15:29:50.482167Z",
          "iopub.status.idle": "2025-07-02T15:29:50.491007Z",
          "shell.execute_reply": "2025-07-02T15:29:50.489927Z",
          "shell.execute_reply.started": "2025-07-02T15:29:50.482412Z"
        },
        "id": "X9sw5nPVtbRW",
        "outputId": "b40f0bb7-7174-40e9-8f54-ffdfeade091e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proposed base_dir: /content/drive/My Drive/xray_data/data\n",
            "Contents of proposed base_dir: ['01', '02', '03', 'train', 'val', 'test']\n"
          ]
        }
      ],
      "source": [
        "# Define the original dataset base directory\n",
        "base_dir = '/content/drive/My Drive/xray_data/data'\n",
        "\n",
        "print(f\"Proposed base_dir: {base_dir}\")\n",
        "print(f\"Contents of proposed base_dir: {os.listdir(base_dir) if os.path.exists(base_dir) else 'Path does not exist.'}\")\n",
        "\n",
        "# Define the names of the subdirectories within base_dir\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "val_dir = os.path.join(base_dir, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHns0XDG0HwG"
      },
      "outputs": [],
      "source": [
        "# def count_images(directory):\n",
        "#     \"\"\"Count images in directory and its subdirectories\"\"\"\n",
        "#     counts = defaultdict(int)\n",
        "#     for root, dirs, files in os.walk(directory):\n",
        "#         for file in files:\n",
        "#             if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "#                 class_name = os.path.basename(root)\n",
        "#                 counts[class_name] += 1\n",
        "#     return dict(counts)\n",
        "\n",
        "# # Count images in each split\n",
        "# train_counts = count_images(train_dir)\n",
        "# val_counts = count_images(val_dir)\n",
        "# test_counts = count_images(test_dir)\n",
        "\n",
        "# # Print results in a clear table format\n",
        "# table = [\n",
        "#     [\"Train\", train_counts.get('NORMAL', 0), train_counts.get('PNEUMONIA', 0), sum(train_counts.values())],\n",
        "#     [\"Validation\", val_counts.get('NORMAL', 0), val_counts.get('PNEUMONIA', 0), sum(val_counts.values())],\n",
        "#     [\"Test\", test_counts.get('NORMAL', 0), test_counts.get('PNEUMONIA', 0), sum(test_counts.values())],\n",
        "#     [\"TOTAL\",\n",
        "#      train_counts.get('NORMAL', 0) + val_counts.get('NORMAL', 0) + test_counts.get('NORMAL', 0),\n",
        "#      train_counts.get('PNEUMONIA', 0) + val_counts.get('PNEUMONIA', 0) + test_counts.get('PNEUMONIA', 0),\n",
        "#      sum(train_counts.values()) + sum(val_counts.values()) + sum(test_counts.values())]\n",
        "# ]\n",
        "\n",
        "# print(tabulate(table,\n",
        "#                headers=[\"Split\", \"NORMAL\", \"PNEUMONIA\", \"Total\"],\n",
        "#                tablefmt=\"grid\",\n",
        "#                numalign=\"center\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LnrfRsYtbRX",
        "outputId": "2eed8df5-e718-425c-a4d4-9632ecf1eca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+----------+----------+----------+---------+\n",
            "| Split      |  Class1  |  Class2  |  Class3  |  Total  |\n",
            "+============+==========+==========+==========+=========+\n",
            "| Train      |    49    |    49    |    77    |   175   |\n",
            "+------------+----------+----------+----------+---------+\n",
            "| Validation |    17    |    17    |    26    |   60    |\n",
            "+------------+----------+----------+----------+---------+\n",
            "| Test       |    17    |    17    |    27    |   61    |\n",
            "+------------+----------+----------+----------+---------+\n",
            "| TOTAL      |    83    |    83    |   130    |   296   |\n",
            "+------------+----------+----------+----------+---------+\n"
          ]
        }
      ],
      "source": [
        "def count_images(directory):\n",
        "    \"\"\"Count images in directory and its subdirectories.\"\"\"\n",
        "    counts = defaultdict(int)\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                class_name = os.path.basename(root)\n",
        "                counts[class_name] += 1\n",
        "    return dict(counts)\n",
        "\n",
        "# Count images in each split\n",
        "train_counts = count_images(train_dir)\n",
        "val_counts = count_images(val_dir)\n",
        "test_counts = count_images(test_dir)\n",
        "\n",
        "# Get all unique class names across splits\n",
        "all_classes = sorted(set(train_counts) | set(val_counts) | set(test_counts))\n",
        "\n",
        "# Prepare the table header dynamically\n",
        "headers = [\"Split\"] + all_classes + [\"Total\"]\n",
        "\n",
        "# Build table rows\n",
        "def build_row(label, counts_dict):\n",
        "    row = [label]\n",
        "    total = 0\n",
        "    for class_name in all_classes:\n",
        "        count = counts_dict.get(class_name, 0)\n",
        "        row.append(count)\n",
        "        total += count\n",
        "    row.append(total)\n",
        "    return row\n",
        "\n",
        "table = [\n",
        "    build_row(\"Train\", train_counts),\n",
        "    build_row(\"Validation\", val_counts),\n",
        "    build_row(\"Test\", test_counts)\n",
        "]\n",
        "\n",
        "# Add total row\n",
        "total_counts = defaultdict(int)\n",
        "for class_name in all_classes:\n",
        "    total_counts[class_name] = (\n",
        "        train_counts.get(class_name, 0)\n",
        "        + val_counts.get(class_name, 0)\n",
        "        + test_counts.get(class_name, 0)\n",
        "    )\n",
        "table.append(build_row(\"TOTAL\", total_counts))\n",
        "\n",
        "# Display the table\n",
        "print(tabulate(table, headers=headers, tablefmt=\"grid\", numalign=\"center\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh0aw2cUtbRX"
      },
      "source": [
        "### Dataset Distribution Analysis\n",
        "\n",
        "The dataset shows significant class imbalance and split distribution issues:\n",
        "\n",
        "1. **Class Imbalance**:\n",
        "   - **Pneumonia cases dominate** (4,273) vs Normal (1,583)\n",
        "\n",
        "2. **Split Problems**:\n",
        "   - **Validation set is critically small** (only 16 images)\n",
        "     - 8 Normal + 8 Pneumonia samples\n",
        "     - Far below the recommended 10-20% of total data\n",
        "\n",
        "3. **Training Implications**:\n",
        "   - The tiny validation set **cannot provide reliable metrics**\n",
        "   - High risk of **overfitting** with no meaningful validation\n",
        "   - Model performance claims will be **statistically unreliable**\n",
        "\n",
        "4. **Recommended Actions**:\n",
        "   - **Redistribute splits** to get at least 500-1000 validation images\n",
        "   - Consider **stratified k-fold cross-validation** instead of fixed splits\n",
        "   - Apply **strong regularization** (dropout, L2, early stopping)\n",
        "   - Use **class weights** to handle the imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zn3XZBUtbRX",
        "outputId": "d7e587cf-f93c-41d0-d659-91956dbde3a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying Class1 to train: 100%|██████████| 29/29 [00:00<00:00, 189.23it/s]\n",
            "Copying Class1 to val: 100%|██████████| 10/10 [00:00<00:00, 220.12it/s]\n",
            "Copying Class1 to test: 100%|██████████| 10/10 [00:00<00:00, 232.27it/s]\n",
            "Copying Class2 to train: 100%|██████████| 29/29 [00:00<00:00, 172.89it/s]\n",
            "Copying Class2 to val: 100%|██████████| 10/10 [00:00<00:00, 77.64it/s]\n",
            "Copying Class2 to test: 100%|██████████| 10/10 [00:00<00:00, 171.54it/s]\n",
            "Copying Class3 to train: 100%|██████████| 46/46 [00:00<00:00, 186.17it/s]\n",
            "Copying Class3 to val: 100%|██████████| 15/15 [00:00<00:00, 74.20it/s]\n",
            "Copying Class3 to test: 100%|██████████| 16/16 [00:00<00:00, 204.27it/s]\n"
          ]
        }
      ],
      "source": [
        "# Define new split ratios (70-15-15)\n",
        "TRAIN_RATIO = 0.6\n",
        "VAL_RATIO = 0.2\n",
        "TEST_RATIO = 0.2\n",
        "\n",
        "def redistribute_data_stratified(base_dir, new_base_dir):\n",
        "    os.makedirs(new_base_dir, exist_ok=True)\n",
        "\n",
        "    for class_name in ['Class1', 'Class2', 'Class3']:\n",
        "        # Collect all image paths\n",
        "        class_path = os.path.join(base_dir, class_name)\n",
        "        images = [os.path.join(class_path, f) for f in os.listdir(class_path)\n",
        "                 if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        # Generate labels (same length as images)\n",
        "        labels = [class_name] * len(images)\n",
        "\n",
        "        # First split: Train (70%) vs Temp (30%)\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "            images, labels,\n",
        "            test_size=(VAL_RATIO + TEST_RATIO),  # 30% for val+test\n",
        "            stratify=labels,  # Critical for class balance\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Second split: Val (15%) and Test (15%) from Temp\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=TEST_RATIO/(VAL_RATIO + TEST_RATIO),  # 0.15/0.3 = 0.5\n",
        "            stratify=y_temp,  # Maintain balance in val/test\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Copy files to new structure\n",
        "        for split, paths in zip(['train', 'val', 'test'], [X_train, X_val, X_test]):\n",
        "            dest_dir = os.path.join(new_base_dir, split, class_name)\n",
        "            os.makedirs(dest_dir, exist_ok=True)\n",
        "            for src in tqdm(paths, desc=f'Copying {class_name} to {split}'):\n",
        "                shutil.copy(src, dest_dir)\n",
        "\n",
        "# Usage\n",
        "redistribute_data_stratified(os.path.join(base_dir, 'train'), 'data_redistributed_stratified')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-aS0PSntbRX"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-02T15:30:14.16052Z",
          "iopub.status.busy": "2025-07-02T15:30:14.160333Z",
          "iopub.status.idle": "2025-07-02T15:30:14.170163Z",
          "shell.execute_reply": "2025-07-02T15:30:14.169431Z",
          "shell.execute_reply.started": "2025-07-02T15:30:14.160485Z"
        },
        "id": "5-b9C3lNtbRY",
        "outputId": "e055a4ce-4e36-498f-a2de-68a5342f66b1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set distribution: {'Class3': 53, 'Class2': 34, 'Class1': 34}\n",
            "Test set distribution: {'Class3': 22, 'Class2': 12, 'Class1': 12}\n",
            "Validation set distribution: {'Class3': 20, 'Class2': 14, 'Class1': 14}\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "base_dir = 'data_redistributed_stratified'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "# Get class distribution\n",
        "def get_class_distribution(directory):\n",
        "    classes = os.listdir(directory)\n",
        "    class_counts = {}\n",
        "    for cls in classes:\n",
        "        class_path = os.path.join(directory, cls)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[cls] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "train_dist = get_class_distribution(train_dir)\n",
        "test_dist = get_class_distribution(test_dir)\n",
        "val_dist = get_class_distribution(val_dir)\n",
        "\n",
        "print(\"Training set distribution:\", train_dist)\n",
        "print(\"Test set distribution:\", test_dist)\n",
        "print(\"Validation set distribution:\", val_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03wI7yuftbRY"
      },
      "source": [
        "After the redistribution we have:\n",
        "\n",
        "- Class ratios are consistent:\n",
        "    - The distribution of PNEUMONIA (≈74%) vs. NORMAL (≈26%) is almost identical across training, test, and validation sets.\n",
        "    - This prevents distribution mismatch, where the model performs well on training but poorly on test/validation.\n",
        "- Reasonable split sizes\n",
        "Training set (~3650 samples) is the largest, which is good for learning.\n",
        "Test (~784) and validation (~782) sets are large enough for reliable evaluation.\n",
        "\n",
        "- Possible **concerns**:\n",
        "\n",
        "    - Class Imbalance (74% vs. 26%). Pneumonia cases dominate, which might bias the model toward predicting \"PNEUMONIA\" more often.\n",
        "        - Solution: Use class weights (e.g., class_weight='balanced' in scikit-learn) or oversample the minority class (NORMAL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9Sklw8TtbRY",
        "outputId": "aa72588a-7292-444e-81a0-335d9f41bdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate images between train and validation found: 14\n",
            "Number of duplicate images between train and test found: 7\n",
            "Number of duplicate images between validation and test found: 23\n"
          ]
        }
      ],
      "source": [
        "def get_image_hashes(directory):\n",
        "    \"\"\"Get hashes of all images in a directory and its subdirectories\"\"\"\n",
        "    hashes = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                try:\n",
        "                    img_path = os.path.join(root, file)\n",
        "                    with Image.open(img_path) as img:\n",
        "                        hashes.append(average_hash(img))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {img_path}: {str(e)}\")\n",
        "    return hashes\n",
        "\n",
        "# Get hashes for all images\n",
        "train_hashes = get_image_hashes(train_dir)\n",
        "val_hashes = get_image_hashes(val_dir)\n",
        "test_hashes = get_image_hashes(test_dir)\n",
        "\n",
        "# Find duplicates\n",
        "duplicates_tr_v = set(train_hashes) & set(val_hashes)\n",
        "print(f\"Number of duplicate images between train and validation found: {len(duplicates_tr_v)}\")\n",
        "duplicates_tr_t = set(train_hashes) & set(test_hashes)\n",
        "print(f\"Number of duplicate images between train and test found: {len(duplicates_tr_t)}\")\n",
        "duplicates_v_t = set(val_hashes) & set(test_hashes)\n",
        "print(f\"Number of duplicate images between validation and test found: {len(duplicates_v_t)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnEukmyitbRY"
      },
      "source": [
        "We checked that, because we faced repeatedely significant overfitting cases across all models and it can lead to data leakage.\n",
        "- Train ↔ Validation(52):\tModel may \"cheat\" by memorizing validation samples\n",
        "- Train ↔ Test(44): Test metrics become unreliable (false high accuracy)\n",
        "- Validation ↔ Test(17): Reduces independence of evaluation sets\n",
        "\n",
        "As we can see we have a serious problem of overlapping between the datasets, which explains the overfitted previous tries. We will identify these pictures and preserve them to the train_dir only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYg-HyOftbRZ"
      },
      "outputs": [],
      "source": [
        "# Create a folder for duplicates\n",
        "duplicates_dir = os.path.join(base_dir, 'duplicates')\n",
        "os.makedirs(duplicates_dir, exist_ok=True)\n",
        "\n",
        "# This will store all unique hashes\n",
        "unique_hashes = set()\n",
        "\n",
        "# Process training set first (keep all training images)\n",
        "for class_name in ['NORMAL', 'PNEUMONIA']:\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "    for img_name in os.listdir(class_path):\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                h = str(average_hash(img))\n",
        "                unique_hashes.add(h)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# Now process validation/test sets and move duplicates to train\n",
        "for split_dir in [val_dir, test_dir]:\n",
        "    for class_name in ['NORMAL', 'PNEUMONIA']:\n",
        "        class_path = os.path.join(split_dir, class_name)\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    h = str(average_hash(img))\n",
        "                    if h in unique_hashes:\n",
        "                        # Move duplicate to train set\n",
        "                        dest_dir = os.path.join(train_dir, class_name)\n",
        "                        shutil.move(img_path, os.path.join(dest_dir, img_name))\n",
        "                    else:\n",
        "                        unique_hashes.add(h)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "print(\"Duplicate removal complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tinVgmSmtbRZ"
      },
      "outputs": [],
      "source": [
        "def count_images(directory):\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(directory):\n",
        "        count += len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    return count\n",
        "\n",
        "print(\"Training images:\", count_images(train_dir))\n",
        "print(\"Validation images:\", count_images(val_dir))\n",
        "print(\"Test images:\", count_images(test_dir))\n",
        "\n",
        "# Get the new distributions\n",
        "print('Train:', get_class_distribution(train_dir))\n",
        "print('Validation:',get_class_distribution(val_dir))\n",
        "print('Test:',get_class_distribution(test_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIm7WWG5tbRZ"
      },
      "outputs": [],
      "source": [
        "# Image Statistics Analysis\n",
        "def analyze_image_stats(directory):\n",
        "    pixel_values = []\n",
        "    sizes = []\n",
        "\n",
        "    for class_name in os.listdir(directory):\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        for img_file in os.listdir(class_path)[:100]:  # Sample 100 images per class\n",
        "            img_path = os.path.join(class_path, img_file)\n",
        "            img = np.array(Image.open(img_path).convert('L'))  # Convert to grayscale\n",
        "\n",
        "            pixel_values.extend(img.flatten())\n",
        "            sizes.append(img.shape)\n",
        "\n",
        "    # Plot histograms\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(pixel_values, bins=50, kde=True)\n",
        "    plt.title('Pixel Value Distribution')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sizes = np.array(sizes)\n",
        "    sns.scatterplot(x=sizes[:,1], y=sizes[:,0])  # Width vs Height\n",
        "    plt.title('Image Dimensions')\n",
        "    plt.xlabel('Width')\n",
        "    plt.ylabel('Height')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Average image size: {np.mean(sizes, axis=0)}\")\n",
        "    print(f\"Pixel value stats - Mean: {np.mean(pixel_values):.1f}, Std: {np.std(pixel_values):.1f}\")\n",
        "\n",
        "analyze_image_stats(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8jp2pRitbRZ"
      },
      "source": [
        "`Image Analysis Summary`\n",
        "\n",
        "### Basic Statistics\n",
        "- **Average Image Size**: 1126.84 (width) × 1465.315 (height) pixels  \n",
        "- **Pixel Value Distribution**:  \n",
        "  - **Mean**: 124.6  \n",
        "  - **Standard Deviation**: 63.4  \n",
        "\n",
        "### Observations\n",
        "- The histogram shows pixel values ranging from 0 to 255, with counts distributed across intensities.  \n",
        "- The mean pixel value (124.6) suggests a balanced distribution, while the standard deviation (63.4) indicates moderate variability in pixel intensities.  \n",
        "\n",
        "### Potential Insights\n",
        "- The image dimensions are large, which may imply high resolution or detailed content.  \n",
        "- The pixel value distribution could be further analyzed to identify dominant intensity ranges or patterns.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYY9xfjctbRZ"
      },
      "outputs": [],
      "source": [
        "# Class Distribution Visualization\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Create subplots\n",
        "for i, (name, dist) in enumerate(zip(['Train', 'Validation', 'Test'],\n",
        "                                   [train_dist, val_dist, test_dist]), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    bars = plt.bar(dist.keys(), dist.values(), color=['skyblue', 'salmon'], alpha=0.8)\n",
        "    plt.title(f'{name} Set\\n({sum(dist.values())} images)')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height)}',\n",
        "                ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout(pad=3.0)  # Add extra padding between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRS9x-KtbRZ"
      },
      "source": [
        "`Dataset Distribution Analysis`\n",
        "\n",
        "Class Distribution Across Splits  \n",
        "**Imbalance**: 75% Normal | 25% Pneumonia   \n",
        "\n",
        "### Key Observations  \n",
        "1. **Uniform Imbalance**:  \n",
        "   - Identical class ratios across splits mitigate bias during training and testing.  \n",
        "2. **Implications**:  \n",
        "   - Models may inherit bias toward the majority class (Normal).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QROEdYFtbRZ"
      },
      "outputs": [],
      "source": [
        "# Image Quality Checks\n",
        "def check_image_quality(directory):\n",
        "    problematic = []\n",
        "    for class_name in os.listdir(directory):\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        for img_file in os.listdir(class_path):\n",
        "            try:\n",
        "                img = Image.open(os.path.join(class_path, img_file))\n",
        "                img.verify()  # Verify integrity\n",
        "                if img.mode != 'L':  # Check if grayscale\n",
        "                    problematic.append((class_name, img_file, 'Color mode issue'))\n",
        "            except Exception as e:\n",
        "                problematic.append((class_name, img_file, str(e)))\n",
        "\n",
        "    return pd.DataFrame(problematic, columns=['Class', 'Filename', 'Issue'])\n",
        "\n",
        "quality_issues = check_image_quality(train_dir)\n",
        "if not quality_issues.empty:\n",
        "    print(\"Found image quality issues:\")\n",
        "    display(quality_issues)\n",
        "else:\n",
        "    print(\"No image quality issues detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjxBHgcKtbRa"
      },
      "source": [
        "`Image Quality Analysis Report`\n",
        "\n",
        "**Summary of Detected Issues**\n",
        "\n",
        "- **Total Problematic Images**: 213  \n",
        "- **Exclusive to Class**: All found in `PNEUMONIA` category  \n",
        "- **Primary Issue**: **Color Mode** (Expected: Grayscale ('L'), Actual: Likely RGB/other)  \n",
        "\n",
        "**Key Observations**\n",
        "1. **Consistency of Issue**:  \n",
        "   - 100% of flagged images are from the pneumonia class, suggesting a systematic data collection/processing artifact.  \n",
        "\n",
        "2. **Potential Impact**:  \n",
        "   - Model training may be affected if:  \n",
        "     - Grayscale normalization is applied uniformly (RGB images carry redundant channels).  \n",
        "     - The color discrepancy correlates with label errors (e.g., mislabeled samples).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSO_wsHtbRa"
      },
      "outputs": [],
      "source": [
        "def plot_sample_images(directory, class_name, num_images=4, figsize=(18, 5)):\n",
        "    class_path = os.path.join(directory, class_name)\n",
        "    image_files = os.listdir(class_path)[:num_images]\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
        "    if num_images == 1:\n",
        "        axes = [axes]  # Ensure axes is always iterable\n",
        "\n",
        "    for i, (ax, image_file) in enumerate(zip(axes, image_files)):\n",
        "        img_path = os.path.join(class_path, image_file)\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_title(f\"{class_name}\\n{image_file}\", pad=10)  # Newline for better spacing\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(pad=2.0)  # Increase padding between subplots\n",
        "    plt.show()\n",
        "\n",
        "# Example usage with improved layout\n",
        "plot_sample_images(train_dir, 'NORMAL', num_images=5, figsize=(20, 4))\n",
        "plot_sample_images(train_dir, 'PNEUMONIA', num_images=5, figsize=(20, 4))\n",
        "\n",
        "# For side-by-side class comparison\n",
        "def plot_class_comparison(directory, num_images=3):\n",
        "    fig, axes = plt.subplots(2, num_images, figsize=(15, 8))\n",
        "\n",
        "    for i, class_name in enumerate(['NORMAL', 'PNEUMONIA']):\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        images = os.listdir(class_path)[:num_images]\n",
        "\n",
        "        for j, img_file in enumerate(images):\n",
        "            ax = axes[i,j]\n",
        "            img = Image.open(os.path.join(class_path, img_file))\n",
        "            ax.imshow(img, cmap='gray')\n",
        "            ax.set_title(f\"{class_name}\\n{img_file}\", fontsize=10)\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(h_pad=2.0, w_pad=1.0)  # Vertical and horizontal padding\n",
        "    plt.show()\n",
        "\n",
        "plot_class_comparison(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqKMdY5tbRa"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcZG1qKotbRa"
      },
      "source": [
        "# Training and Validation Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSjSzx1ItbRa"
      },
      "source": [
        "## Basic ML (HOG: Feature Extraction & Random Forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MczszW1QtbRa"
      },
      "source": [
        "`Why Choose Random Forest as a Baseline Model for Pneumonia vs. Normal Classification?`\n",
        "\n",
        "**Random Forest** is often chosen as a baseline model in medical imaging tasks like pneumonia detection for several important reasons:\n",
        "\n",
        "- **Fast to Train and Easy to Use**  \n",
        "  Random Forest requires minimal preprocessing and trains quickly on tabular or feature-based data, making it a great initial benchmark.\n",
        "\n",
        "- **Strong Performance on Extracted Features**  \n",
        "  When working with engineered features or embeddings (e.g., radiomic features or CNN-extracted features), Random Forest handles tabular data very well.\n",
        "\n",
        "- **Interpretable Results**  \n",
        "  It provides insight into feature importance and decision paths, which is valuable in clinical settings where explainability is crucial.\n",
        "\n",
        "- **Robust to Overfitting**  \n",
        "  As an ensemble of decision trees, Random Forest averages predictions to reduce overfitting compared to individual trees or small neural networks.\n",
        "\n",
        "- **Capable of Learning Non-Linear Patterns**  \n",
        "  It can capture complex relationships in the data that help differentiate pneumonia from normal cases.\n",
        "\n",
        "- **Good Initial Benchmark**  \n",
        "  Before training computationally intensive deep learning models, Random Forest gives a quick indication of how informative the features are.\n",
        "\n",
        "**Limitations:**  \n",
        "Random Forest is not designed for raw image data and performs best when used on extracted features rather than raw pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T15:30:15.886173Z",
          "iopub.status.busy": "2025-07-02T15:30:15.88573Z",
          "iopub.status.idle": "2025-07-02T15:31:58.912161Z",
          "shell.execute_reply": "2025-07-02T15:31:58.911319Z",
          "shell.execute_reply.started": "2025-07-02T15:30:15.885935Z"
        },
        "id": "mEIeYhg8tbRa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def extract_hog_features(image_path, resize_dim=(128, 128)):\n",
        "    img = Image.open(image_path).convert('L')\n",
        "    img = img.resize(resize_dim)\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Compute HOG features\n",
        "    fd, hog_image = hog(\n",
        "        img_array,\n",
        "        orientations=8,\n",
        "        pixels_per_cell=(16, 16),\n",
        "        cells_per_block=(1, 1),\n",
        "        visualize=True\n",
        "    )\n",
        "\n",
        "    return fd\n",
        "\n",
        "# Prepare dataset for classical ML\n",
        "def prepare_classical_ml_data(directory):\n",
        "    X = []\n",
        "    y = []\n",
        "    class_names = os.listdir(directory)\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        for image_file in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_file)\n",
        "            features = extract_hog_features(image_path)\n",
        "            X.append(features)\n",
        "            y.append(0 if class_name == 'NORMAL' else 1)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepare data\n",
        "X_train, y_train = prepare_classical_ml_data(train_dir)\n",
        "X_test, y_test = prepare_classical_ml_data(test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ53ytTZtbRa"
      },
      "source": [
        "`HOG Feature Extraction for Classical ML`\n",
        "\n",
        "**Key Components:**\n",
        "1. `extract_hog_features()` Function:\n",
        "   - Converts images to grayscale (`'L'` mode)\n",
        "   - Resizes to consistent dimensions (default 128×128)\n",
        "   - Computes Histogram of Oriented Gradients (HOG) features with:\n",
        "     \n",
        "2. `prepare_classical_ml_data()` Function:\n",
        "   - Processes directory structure to:\n",
        "     - Extract HOG features per image\n",
        "     - Assign labels (0 for `NORMAL`, 1 for `PNEUMONIA`)\n",
        "   - Returns feature matrix `X` and label vector `y`\n",
        "\n",
        "`HOG Parameters Explained:`\n",
        "| Parameter          | Value    | Effect                                                                 |\n",
        "|--------------------|----------|-----------------------------------------------------------------------|\n",
        "| `orientations`     | 8        | Number of gradient orientation bins (trade-off between detail and dimensionality) |\n",
        "| `pixels_per_cell`  | (16,16)  | Smaller values capture finer details but increase feature size        |\n",
        "| `cells_per_block`  | (1,1)    | No block normalization (simpler features)                             |\n",
        "\n",
        "\n",
        "`Key Advantages for Pneumonia Detection`\n",
        "\n",
        "1. **Shape and Texture Sensitivity**\n",
        "   - HOG excels at capturing **edge patterns** and **local shape information**\n",
        "   - Critical for detecting:\n",
        "     - Pulmonary infiltrates (pneumonia)\n",
        "     - Alveolar patterns\n",
        "     - Bronchial wall thickening\n",
        "\n",
        "2. **Illumination Invariance**\n",
        "   - Normalizes gradients → robust to:\n",
        "     - X-ray exposure variations\n",
        "     - Scanner differences\n",
        "     - Contrast fluctuations\n",
        "\n",
        "3. **Dimensionality Efficiency**\n",
        "    - The configuration (512 features) is:\n",
        "     - **Rich enough**: Captures pulmonary pathology signatures\n",
        "     - **Compact enough**: Avoids curse of dimensionality vs. raw pixels (16,384 for 128×128)\n",
        "\n",
        "4. **Clinical Interpretability**\n",
        "   - Visualizing HOG features shows:\n",
        "     - Where the model detects important edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T15:31:58.913713Z",
          "iopub.status.busy": "2025-07-02T15:31:58.913504Z",
          "iopub.status.idle": "2025-07-02T15:32:04.639936Z",
          "shell.execute_reply": "2025-07-02T15:32:04.639287Z",
          "shell.execute_reply.started": "2025-07-02T15:31:58.913678Z"
        },
        "id": "8QuCIfkptbRb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Train Random Forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "print(\"Random Forest Classifier Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_umy4XZtbRb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQmFw4oCtbRb"
      },
      "source": [
        "`Data Augmentation Techniques`\n",
        "\n",
        "- **Rescaling:** Normalize pixel values to [0,1] for faster and more stable training.\n",
        "- **Rotation:** Randomly rotate images up to ±8 degrees to improve rotational invariance.\n",
        "- **Width & Height Shift:** Slightly shift images horizontally and vertically (up to 5%) to simulate positional variance.\n",
        "- **Zoom:** Randomly zoom in/out within 5% to enhance scale robustness.\n",
        "- **Brightness:** Slight brightness adjustments (±5%) to increase lighting variation.\n",
        "- **Horizontal Flip:** Randomly flip images horizontally to augment data diversity.\n",
        "- **Fill Mode:** Use nearest pixel values to fill empty areas created during transformations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0apn_lxqtbRb"
      },
      "source": [
        "## Custom CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H2iQeMxtbRb"
      },
      "outputs": [],
      "source": [
        "# Image dimensions\n",
        "img_height, img_width = 320, 320 # decided that based on the statistics we produced above\n",
        "batch_size = 32\n",
        "input_shape = (img_height, img_width, 1)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# DINT'T WORK\n",
        "\n",
        "# mean_pixel = 124.6\n",
        "# std_pixel = 63.4\n",
        "# preprocessing_function = lambda x: (x - mean_pixel) / std_pixel\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "# Training Generator\n",
        "train_datagen_gray = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=8,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    zoom_range=0.05,\n",
        "    brightness_range=[0.95, 1.05],\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation/Test Generator (No augmentation)\n",
        "val_test_datagen_gray = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Create generators\n",
        "train_generator = train_datagen_gray.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    color_mode='grayscale',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_generator_gray = val_test_datagen_gray.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    color_mode='grayscale',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator_gray = val_test_datagen_gray.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=1,\n",
        "    class_mode='binary',\n",
        "    color_mode='grayscale',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbHWkuzBtbRc"
      },
      "outputs": [],
      "source": [
        "# Plot some images to ensure that the augmentation does NOT change the pics too much\n",
        "x_batch, y_batch = next(train_generator)\n",
        "\n",
        "for i in range(5):\n",
        "    plt.imshow(x_batch[i].squeeze(), cmap='gray')\n",
        "    plt.title(f\"Label: {y_batch[i]}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfMaH-90tbRc"
      },
      "outputs": [],
      "source": [
        "# Important due to the significance imbalance issues\n",
        "\n",
        "y_train_labels = train_generator.classes\n",
        "\n",
        "# Automatically compute class weights\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_labels),\n",
        "    y=y_train_labels\n",
        ")\n",
        "\n",
        "# Convert to dictionary for model.fit\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weight_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfXamiLUtbRf"
      },
      "source": [
        "We have significant imbalanced set, so we need to take it into account for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXdX4LjNtbRf"
      },
      "outputs": [],
      "source": [
        "# Build Model with Batch Normalization\n",
        "cnn_model = Sequential([\n",
        "    # Feature extraction\n",
        "    Conv2D(16, (3, 3), padding='same', input_shape=(img_height, img_width, 1), kernel_regularizer=regularizers.l2(0.0005)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdLLp3YctbRg"
      },
      "outputs": [],
      "source": [
        "# Compile Model\n",
        "cnn_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "cnn_callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        mode='min',\n",
        "        restore_best_weights=True,\n",
        "        min_delta=0.005\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        'best_weights.keras',\n",
        "        monitor='val_auc',\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ncMmnVtbRg"
      },
      "source": [
        "`Custom CNN Model Architecture`\n",
        "\n",
        "- Input: Grayscale images with shape (height, width, 1).\n",
        "- Three convolutional blocks with Conv2D layers (16, 64, 128 filters), each followed by Batch Normalization, ReLU activation, and MaxPooling.\n",
        "- Dropout (0.3) after the last convolution block for regularization.\n",
        "- Flatten layer to convert 3D features to 1D vector.\n",
        "- Dense layer with 32 units, L2 regularization, Batch Normalization, ReLU activation, and Dropout (0.5).\n",
        "- Output layer with a single sigmoid neuron for binary classification.\n",
        "- Compiled with Adam optimizer, binary cross-entropy loss, and accuracy metric.\n",
        "- Callbacks include early stopping, model checkpointing (monitoring validation AUC), and learning rate reduction on plateau.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mETIiXH0tbRg"
      },
      "outputs": [],
      "source": [
        "# Train Model\n",
        "history = cnn_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator_gray,\n",
        "    epochs=30,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=cnn_callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhZF_KS4tbRg"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curves(history):\n",
        "    # Accuracy\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-02T15:43:27.76123Z",
          "iopub.status.idle": "2025-07-02T15:43:27.761693Z",
          "shell.execute_reply": "2025-07-02T15:43:27.761377Z"
        },
        "id": "d9jo2nSvtbRg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Call the function\n",
        "plot_learning_curves(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-02T15:43:27.762258Z",
          "iopub.status.idle": "2025-07-02T15:43:27.762811Z",
          "shell.execute_reply": "2025-07-02T15:43:27.762498Z"
        },
        "id": "0BFgkx71tbRg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Evaluate Model on Test Set\n",
        "print(\"\\n--- Evaluating the model on the test set ---\")\n",
        "test_loss, test_acc = cnn_model.evaluate(test_generator_gray)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuRPvT6wtbRg"
      },
      "outputs": [],
      "source": [
        "# Save Final Model\n",
        "cnn_model.save('final_model.h5')\n",
        "print(\"Final model saved as final_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1KGOb4GtbRg"
      },
      "source": [
        "`Training and Evaluation Results`\n",
        "\n",
        "- Training accuracy improved steadily, reaching ~94% by epoch 10.\n",
        "- Validation accuracy peaked at ~96.7% (epoch 5) but showed some fluctuation afterward.\n",
        "- Validation loss decreased significantly early on, indicating good learning, but later epochs showed some increase, triggering learning rate reduction.\n",
        "- Early stopping prevented overfitting by restoring best weights at patience.\n",
        "- Final evaluation on the test set yielded very high accuracy (~97.6%) and low loss (~0.138), confirming strong generalization.\n",
        "\n",
        "**Actions Taken:**\n",
        "- Used stratified splitting for train/validation/test to preserve class balance.\n",
        "- Applied early stopping and learning rate scheduling to avoid overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWLQsUEvtbRg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPDSdlaXtbRh"
      },
      "source": [
        "## VGG16 pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWMpf1Z0tbRh"
      },
      "source": [
        "`Why Choose VGG16 for Pneumonia vs. Normal Classification`\n",
        "\n",
        "1. **Proven Performance on Medical Imaging**\n",
        "\n",
        "VGG16 has been widely used in medical imaging tasks, especially where features are localized and textural, like pneumonia patterns in chest X-rays.\n",
        "\n",
        "2. **Pretrained on ImageNet**\n",
        "\n",
        "VGG16 is trained on ImageNet (1M+ images across 1000 classes), so its early layers learn low-level features (edges, blobs, shapes) that are general enough to transfer well to grayscale or radiology images.\n",
        "\n",
        "Even though X-rays are grayscale, they are usually converted to 3 channels (RGB triplicates) to match the input shape required by VGG16.\n",
        "\n",
        "3. **Simple and Interpretable Architecture**\n",
        "\n",
        "VGG16 has a simple and consistent architecture: stacks of 3×3 convolutional filters followed by max-pooling. This makes it:\n",
        "- Easier to debug and modify\n",
        "- More interpretable than more complex models (e.g., ResNet or Inception)\n",
        "\n",
        "4. **Fewer Parameters to Tune**\n",
        "\n",
        "Compared to deeper networks like ResNet50 or InceptionV3, VGG16 has fewer layers and a simpler structure, which:\n",
        "- Makes it easier to train with limited data\n",
        "- Reduces the risk of overfitting when data augmentation is not used\n",
        "\n",
        "5. **Good Trade-off for Moderate Datasets**\n",
        "\n",
        "Working with a moderately sized dataset (like <10,000 X-ray images), VGG16 provides a good balance between:\n",
        "- Model capacity\n",
        "- Speed of training\n",
        "- Risk of overfitting\n",
        "\n",
        "6. **Compatibility with Transfer Learning**\n",
        "\n",
        "Because VGG16 is widely used, it is well-supported in TensorFlow/Keras, and has many pretrained weights available.\n",
        "Transfer learning allows faster convergence and better performance, even on limited datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-02T15:43:27.766247Z",
          "iopub.status.idle": "2025-07-02T15:43:27.766635Z",
          "shell.execute_reply": "2025-07-02T15:43:27.76641Z"
        },
        "id": "1NJjpTTVtbRh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Prepare Data Generators with Augmentation\n",
        "\n",
        "img_height, img_width = 320, 320 # decided that based on the statistics we produced above\n",
        "batch_size = 32\n",
        "input_shape = (img_height, img_width, 1)\n",
        "\n",
        "# VGG16 expects 3-channel input, so change color_mode to 'rgb'\n",
        "# Training Generator\n",
        "train_datagen_rgb = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=8,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    zoom_range=0.05,\n",
        "    brightness_range=[0.95, 1.05],\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',  # Black borders\n",
        ")\n",
        "\n",
        "val_test_datagen_rgb = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create data generators\n",
        "train_generator = train_datagen_rgb.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb', # <--- IMPORTANT: Changed to RGB for VGG16\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_generator_rgb = val_test_datagen_rgb.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb', # <--- IMPORTANT: Changed to RGB for VGG16\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator_rgb = val_test_datagen_rgb.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb', # <--- IMPORTANT: Changed to RGB for VGG16\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-02T15:43:27.76778Z",
          "iopub.status.idle": "2025-07-02T15:43:27.768183Z",
          "shell.execute_reply": "2025-07-02T15:43:27.767936Z"
        },
        "id": "jxk1yijptbRh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Build Model with VGG16 Transfer Learning\n",
        "\n",
        "# Load pre-trained VGG16 without top layers\n",
        "vgg16_model = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(img_height, img_width, 3)  # RGB input\n",
        ")\n",
        "\n",
        "# Freeze VGG16 layers for feature extraction\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "inputs = Input(shape=(img_height, img_width, 3))\n",
        "\n",
        "x = vgg16_model(inputs, training=False)  # Freeze backbone\n",
        "x = GlobalAveragePooling2D()(x)         # Replaces Flatten (reduces params)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = BatchNormalization()(x)             # Improves stability\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Final model\n",
        "vgg_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "vgg_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Set up callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint('vgg16_best_model.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "vgg_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIrkebWStbRh"
      },
      "source": [
        "`VGG16 Transfer Learning Model Architecture`\n",
        "\n",
        "- Uses pre-trained VGG16 convolutional base without the top classification layers.\n",
        "- The VGG16 layers are frozen to leverage learned features and prevent training updates.\n",
        "- Adds a Global Average Pooling layer to reduce feature maps to a vector.\n",
        "- Includes a fully connected Dense layer with 128 units and ReLU activation.\n",
        "- Batch Normalization and Dropout (0.5) are applied for training stability and regularization.\n",
        "- Final output layer uses a single sigmoid neuron for binary classification (pneumonia detection).\n",
        "- Compiled with Adam optimizer, binary cross-entropy loss, and accuracy metric.\n",
        "- Includes callbacks for early stopping, model checkpointing, and learning rate reduction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-02T15:43:27.768852Z",
          "iopub.status.idle": "2025-07-02T15:43:27.769224Z",
          "shell.execute_reply": "2025-07-02T15:43:27.769028Z"
        },
        "id": "m8BaB2hytbRh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Train Model\n",
        "epochs = 20 # Reduced epochs as transfer learning converges faster\n",
        "\n",
        "print(\"\\n--- Starting VGG16 Model Training ---\")\n",
        "history = vgg_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator_rgb,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "print(\"--- VGG16 Model Training Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-02T15:43:27.770005Z",
          "iopub.status.idle": "2025-07-02T15:43:27.770352Z",
          "shell.execute_reply": "2025-07-02T15:43:27.770172Z"
        },
        "id": "VaFfKr1RtbRh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Learning Curve\n",
        "plot_learning_curves(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-02T15:43:27.770836Z",
          "iopub.status.idle": "2025-07-02T15:43:27.771171Z",
          "shell.execute_reply": "2025-07-02T15:43:27.770998Z"
        },
        "id": "q6nGDyfutbRh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Evaluate Model on Test Set\n",
        "print(\"\\n--- Evaluating the VGG16 model on the test set ---\")\n",
        "test_loss, test_accuracy = vgg_model.evaluate(test_generator_rgb, steps=test_generator_rgb.samples // batch_size)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdtcZ0ebtbRh"
      },
      "source": [
        "### Model Evaluation Summary (VGG16)\n",
        "\n",
        "**Training Summary:**\n",
        "- The model reached its **best validation performance at Epoch 7** with:\n",
        "  - `accuracy = 87.43%`, `loss = 0.2583`\n",
        "  - `val_accuracy = 92.89%`, `val_loss = 0.1832`\n",
        "- After this point, **validation loss began to increase** while training accuracy continued improving, indicating **overfitting**.\n",
        "- The learning rate was reduced by `ReduceLROnPlateau` after Epoch 10 due to stagnant validation loss.\n",
        "- **Early stopping** was triggered at Epoch 12, and the model was restored to the weights from Epoch 7 (the best epoch).\n",
        "\n",
        "**Test Performance:**\n",
        "- **Test Accuracy**: **95.45%**\n",
        "- **Test Loss**: **0.1632**\n",
        "\n",
        "These results confirm that the model **generalized well to unseen data**, with test accuracy even slightly surpassing the best validation accuracy. The model shows **strong discriminative ability** for pneumonia detection on chest X-rays.\n",
        "\n",
        "**Training Strategies Applied:**\n",
        "- **ReduceLROnPlateau** dynamically adjusted learning rate based on validation loss stagnation.\n",
        "- **Early stopping** prevented further overfitting and restored the best performing weights.\n",
        "- Training was **terminated at optimal point** (Epoch 7), where generalization was maximized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-02T15:43:27.77186Z",
          "iopub.status.idle": "2025-07-02T15:43:27.772268Z",
          "shell.execute_reply": "2025-07-02T15:43:27.772055Z"
        },
        "id": "wjuUfH8CtbRi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save Final Model\n",
        "vgg_model.save('final_vgg_model.h5')\n",
        "print(\"Final VGG16 model saved as final_vgg_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlp4KkoQtbRi"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU38q_VntbRi"
      },
      "source": [
        "# Evaluation phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPtHcaoFtbRi"
      },
      "source": [
        "## Get Predicitons and Probablities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PDKvQHetbRi"
      },
      "outputs": [],
      "source": [
        "# For Custom CNN (Grayscale)\n",
        "cnn_probs = cnn_model.predict(test_generator_gray).flatten()\n",
        "cnn_preds = (cnn_probs > 0.5).astype(int)\n",
        "y_true_cnn = test_generator_gray.classes\n",
        "\n",
        "# For VGG16 (RGB)\n",
        "vgg_probs = vgg_model.predict(test_generator_rgb).flatten()\n",
        "vgg_preds = (vgg_probs > 0.5).astype(int)\n",
        "y_true_vgg = test_generator_rgb.classes\n",
        "\n",
        "# For Random Forest\n",
        "rf_probs = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "rf_preds = rf_classifier.predict(X_test)\n",
        "y_test_rf = y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAbNjFSktbRi"
      },
      "outputs": [],
      "source": [
        "# Bootstrapping for statistical significance\n",
        "def bootstrap_evaluate(y_true, y_pred, y_prob=None, n_bootstraps=100):\n",
        "    metrics = {\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1': []\n",
        "    }\n",
        "\n",
        "    if y_prob is not None:\n",
        "        metrics['roc_auc'] = []\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        indices = resample(np.arange(len(y_true)))\n",
        "        y_true_boot = y_true[indices]\n",
        "        y_pred_boot = y_pred[indices]\n",
        "\n",
        "        metrics['accuracy'].append(accuracy_score(y_true_boot, y_pred_boot))\n",
        "        metrics['precision'].append(precision_score(y_true_boot, y_pred_boot))\n",
        "        metrics['recall'].append(recall_score(y_true_boot, y_pred_boot))\n",
        "        metrics['f1'].append(f1_score(y_true_boot, y_pred_boot))\n",
        "\n",
        "        if y_prob is not None:\n",
        "            y_prob_boot = y_prob[indices]\n",
        "            metrics['roc_auc'].append(roc_auc_score(y_true_boot, y_prob_boot))\n",
        "\n",
        "    # Calculate statistics\n",
        "    results = {}\n",
        "    for metric, values in metrics.items():\n",
        "        results[metric] = {\n",
        "            'mean': np.mean(values),\n",
        "            'ci_low': np.percentile(values, 2.5),\n",
        "            'ci_high': np.percentile(values, 97.5)\n",
        "        }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86uKzIbLtbRi"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sDC-1G0tbRi"
      },
      "outputs": [],
      "source": [
        "# Evaluate all models\n",
        "models = [\n",
        "    {'name': 'Custom CNN', 'y_true': y_true_cnn, 'y_pred': cnn_preds, 'y_prob': cnn_probs},\n",
        "    {'name': 'VGG16', 'y_true': y_true_vgg, 'y_pred': vgg_preds, 'y_prob': vgg_probs},\n",
        "    {'name': 'Random Forest', 'y_true': y_test_rf, 'y_pred': rf_preds, 'y_prob': rf_probs}\n",
        "]\n",
        "\n",
        "results = []\n",
        "for model in models:\n",
        "    print(f\"\\nEvaluating {model['name']}...\")\n",
        "    boot_results = bootstrap_evaluate(\n",
        "        model['y_true'],\n",
        "        model['y_pred'],\n",
        "        model['y_prob']\n",
        "    )\n",
        "\n",
        "    # Format results\n",
        "    model_results = {'Model': model['name']}\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
        "        if metric in boot_results:\n",
        "            model_results[metric] = f\"{boot_results[metric]['mean']:.3f} ({boot_results[metric]['ci_low']:.3f}-{boot_results[metric]['ci_high']:.3f})\"\n",
        "        else:\n",
        "            model_results[metric] = \"N/A\"\n",
        "\n",
        "    results.append(model_results)\n",
        "\n",
        "# Display results table\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nFinal Results with 95% Confidence Intervals:\")\n",
        "print(results_df.to_markdown(index=False, floatfmt=\".3f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDh6UcHLtbRi"
      },
      "outputs": [],
      "source": [
        "# Mteric Comparison Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
        "x_pos = np.arange(len(metrics_to_plot))\n",
        "colors = ['#8dd3c7', '#feccd5', '#bebada']\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    boot_results = bootstrap_evaluate(model['y_true'], model['y_pred'], model['y_prob'])\n",
        "\n",
        "    means = [boot_results[metric]['mean'] for metric in metrics_to_plot]\n",
        "    ci_lows = [boot_results[metric]['ci_low'] for metric in metrics_to_plot]\n",
        "    ci_highs = [boot_results[metric]['ci_high'] for metric in metrics_to_plot]\n",
        "\n",
        "    plt.bar(x_pos + i*0.25, means, yerr=[np.array(means)-np.array(ci_lows), np.array(ci_highs)-np.array(means)],\n",
        "            width=0.25, color=colors[i], capsize=5, label=model['name'])\n",
        "\n",
        "plt.xticks(x_pos + 0.25, [m.capitalize() for m in metrics_to_plot])\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Comparison with 95% Confidence Intervals')\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OodjJF15tbRi"
      },
      "source": [
        "### Comments:\n",
        "\n",
        "- **Custom CNN** achieved the **highest overall performance** across all metrics, particularly excelling in **recall (0.993)** and **ROC AUC (0.998)**, indicating strong ability to detect pneumonia cases with very few false negatives.\n",
        "- **VGG16**, despite having the **highest precision (0.992)**, shows slightly lower **recall (0.948)**, suggesting it may miss more true pneumonia cases compared to the CNN or RF models.\n",
        "- **Random Forest** performs robustly across all metrics with a strong balance between precision and recall and nearly matches the CNN in ROC AUC, making it a competitive classical ML alternative.\n",
        "\n",
        "`Conclusion:` While all models perform strongly, the **Custom CNN** provides the best trade-off between sensitivity and specificity and would be the most reliable for deployment in clinical screening contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOPycf8ItbRj"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrices\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, model in enumerate(models):\n",
        "    cm = confusion_matrix(model['y_true'], model['y_pred'])\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Normal', 'Pneumonia'],\n",
        "                yticklabels=['Normal', 'Pneumonia'])\n",
        "    plt.title(f'{model[\"name\"]} Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lxnM-XwtbRj"
      },
      "source": [
        "Very few misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82lwPjMItbRj"
      },
      "outputs": [],
      "source": [
        "# ROC Curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "for model in models:\n",
        "    if model['y_prob'] is not None:\n",
        "        fpr, tpr, _ = roc_curve(model['y_true'], model['y_prob'])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2,\n",
        "                 label=f'{model[\"name\"]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySAa3XBltbRj"
      },
      "source": [
        "Nearly perfect scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq63X__btbRj"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmInndL3tbRj"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeOBCTPgtbRj"
      },
      "source": [
        "### Key Findings\n",
        "\n",
        "1. **Model Performance Comparison**:\n",
        "   - The **Custom CNN** achieved the highest overall performance with:\n",
        "     - Exceptional recall (0.993) - critical for medical diagnosis where false negatives are costly\n",
        "     - Near-perfect ROC AUC (0.998) - indicating excellent class separation\n",
        "   - **VGG16** showed slightly lower performance but maintained strong precision (0.992)\n",
        "   - **Random Forest** performed surprisingly well given its simplicity, suggesting HOG features effectively capture pneumonia patterns\n",
        "\n",
        "2. **Clinical Implications**:\n",
        "   - The Custom CNN's high recall makes it suitable for initial screening where sensitivity is prioritized\n",
        "   - VGG16's high precision could be valuable in confirmatory testing scenarios\n",
        "   - Random Forest provides a computationally efficient alternative when GPU resources are limited\n",
        "\n",
        "3. **Dataset Challenges**:\n",
        "   - Initial class imbalance (3:1 pneumonia:normal ratio) was successfully addressed through:\n",
        "     - Class weighting\n",
        "     - Stratified sampling\n",
        "   - Duplicate images across splits were identified and removed, preventing data leakage\n",
        "\n",
        "### Limitations\n",
        "\n",
        "1. **Computational Constraints**:\n",
        "   - VGG16's larger size (56MB vs CNN's 25MB) required more training time\n",
        "   - Image size (320×320) was smaller than original radiographs, potentially losing fine details\n",
        "\n",
        "2. **Clinical Validation**:\n",
        "   - Performance metrics are based on image labels only\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The Custom CNN emerged as the most effective model for this binary classification task, demonstrating that carefully designed architectures can outperform transfer learning approaches in specialized medical imaging domains when sufficient training data is available. The results suggest strong potential for AI-assisted pneumonia screening in clinical settings.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 17810,
          "sourceId": 23812,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "ImageAnalysis_venv_py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
